{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e217d3ee",
      "metadata": {
        "id": "e217d3ee"
      },
      "source": [
        "# Desafío 03"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89133761",
      "metadata": {
        "id": "89133761"
      },
      "source": [
        "## Integrantes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd6a5ae",
      "metadata": {
        "id": "3dd6a5ae"
      },
      "source": [
        "- Acevedo Zain, Gaspar (acevedo.zain.gaspar@gmail.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "947a6938",
      "metadata": {
        "id": "947a6938"
      },
      "source": [
        "## Consignas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090159ca",
      "metadata": {
        "id": "090159ca"
      },
      "source": [
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "***Sugerencias***\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "0ts0cZ-yAt9Q"
      },
      "id": "0ts0cZ-yAt9Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "import platform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.special import softmax\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "iTvdqC2eAu0t"
      },
      "id": "iTvdqC2eAu0t",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import bs4 as bs"
      ],
      "metadata": {
        "id": "XwHsZC7zBAtg"
      },
      "id": "XwHsZC7zBAtg",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selección del corpus"
      ],
      "metadata": {
        "id": "AxXf47bgDCe9"
      },
      "id": "AxXf47bgDCe9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de esta práctica es evaluar modelos de lenguajes con *tokenización por caracteres*, por lo cual, un texto lo suficientemente grande puede servir como `Corpus`.\n",
        "\n",
        "Se elige entonces el libro `La Odisea` de Homero ([source](https://www.textos.info/homero/odisea/ebook)) como `Corpus`."
      ],
      "metadata": {
        "id": "7j9Kc8P5EYmN"
      },
      "id": "7j9Kc8P5EYmN"
    },
    {
      "cell_type": "code",
      "source": [
        "odisea_url = \"https://www.textos.info/homero/odisea/ebook\""
      ],
      "metadata": {
        "id": "YpDwlz-1DI2b"
      },
      "id": "YpDwlz-1DI2b",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_html = urllib.request.urlopen(odisea_url)\n",
        "raw_html = raw_html.read()"
      ],
      "metadata": {
        "id": "EMjWOkgyDGJZ"
      },
      "id": "EMjWOkgyDGJZ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se procesa el *html* original mediante la utilidad `bs.BeautifulSoup` a fin de tener el texto en ***un solo string***."
      ],
      "metadata": {
        "id": "1Tl0ibE1Ek6_"
      },
      "id": "1Tl0ibE1Ek6_"
    },
    {
      "cell_type": "code",
      "source": [
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:\n",
        "    article_text += para.text + ' '\n",
        "\n",
        "article_text = article_text.lower()"
      ],
      "metadata": {
        "id": "tDS_17kRDPly"
      },
      "id": "tDS_17kRDPly",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se muestran los primeros $500$ caracteres del texto (corpus)."
      ],
      "metadata": {
        "id": "0KtVmm1pE0Yl"
      },
      "id": "0KtVmm1pE0Yl"
    },
    {
      "cell_type": "code",
      "source": [
        "article_text[:500]"
      ],
      "metadata": {
        "id": "ebdZ14dJEKtS",
        "outputId": "eaf98086-8461-41ac-9484-c3bf3a7fe702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "id": "ebdZ14dJEKtS",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' háblame, musa, de aquel varón de multiforme ingenio que, después de \\ndestruir la sacra ciudad de troya, anduvo peregrinando larguísimo \\ntiempo, vio las poblaciones y conoció las costumbres de muchos hombres y\\n padeció en su ánimo gran número de trabajos en su navegación por el \\nponto, en cuanto procuraba salvar su vida y la vuelta de sus compañeros a\\n la patria. mas ni aun así pudo librarlos, como deseaba, y todos \\nperecieron por sus propias locuras. ¡insensatos! comiéronse las vacas de\\n helios'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el texto se observan algunas *secuencias de escape de caracteres* (salto de línea o `\\n`), por lo cual se las reemplaza a continuación con un caracter vacío."
      ],
      "metadata": {
        "id": "m-iRtdiiFkn5"
      },
      "id": "m-iRtdiiFkn5"
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = str.replace(article_text, \"\\n\", \"\")"
      ],
      "metadata": {
        "id": "Dr18eaNqESzo"
      },
      "id": "Dr18eaNqESzo",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_text[:500]"
      ],
      "metadata": {
        "id": "INsuGeqCF22a",
        "outputId": "0110ca61-2f06-4f0e-80bf-39dbdb19e2ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "id": "INsuGeqCF22a",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' háblame, musa, de aquel varón de multiforme ingenio que, después de destruir la sacra ciudad de troya, anduvo peregrinando larguísimo tiempo, vio las poblaciones y conoció las costumbres de muchos hombres y padeció en su ánimo gran número de trabajos en su navegación por el ponto, en cuanto procuraba salvar su vida y la vuelta de sus compañeros a la patria. mas ni aun así pudo librarlos, como deseaba, y todos perecieron por sus propias locuras. ¡insensatos! comiéronse las vacas de helios, hijo '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición del vocabulario + Tokenización"
      ],
      "metadata": {
        "id": "mNB4tuhlAO8Y"
      },
      "id": "mNB4tuhlAO8Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección definiremos nuestro `vocabulario` a partir del corpus original.\n",
        "\n",
        "Luego lo tokenizaremos, a fin de que pueda ser procesado por una Red Neuronal en pasos posteriores."
      ],
      "metadata": {
        "id": "U8sqaDkuFMwF"
      },
      "id": "U8sqaDkuFMwF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos definiendo nuestro vocabulario, que es el conjunto de distintos caracteres que aparecen en nuestro corpus.\n",
        "\n",
        "Para el caso particular del texto seleccionado el tamaño es de $57$."
      ],
      "metadata": {
        "id": "Z68zktzNDgtS"
      },
      "id": "Z68zktzNDgtS"
    },
    {
      "cell_type": "code",
      "source": [
        "char_vocab = set(article_text)\n",
        "print(f\"Tamaño del vocabulario: {len(char_vocab)}\")"
      ],
      "metadata": {
        "id": "gonFUApiCVHF",
        "outputId": "633fc937-c6a6-46bb-fd60-d96ac8b78157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gonFUApiCVHF",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del vocabulario: 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos dos diccionarios que se utilizarán a lo largo de este trabajo:\n",
        "- `char2idx`: A cada caracter de nuestro vocabulario se le asigna un `índice`.\n",
        "- `idx2char`: Es el *inverso* de `char2idx`, es decir, dado un índice, me devuelve el caracter correspondiente."
      ],
      "metadata": {
        "id": "dVCdbgxXDw6A"
      },
      "id": "dVCdbgxXDw6A"
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {k: v for v,k in enumerate(char_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}"
      ],
      "metadata": {
        "id": "bTzeda8sDHSF"
      },
      "id": "bTzeda8sDHSF",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora `tokenizamos` el corpus. Para ello hacemos uso del diccionario `char2idx`, reemplazando cada caracter por su índice correspondiente."
      ],
      "metadata": {
        "id": "YiZH4Yt7EVJc"
      },
      "id": "YiZH4Yt7EVJc"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = [char2idx[ch] for ch in article_text]"
      ],
      "metadata": {
        "id": "TN0z8xKbEI4H"
      },
      "id": "TN0z8xKbEI4H",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para validar, mostramos primero los $10$ primero caracteres tokenizados del corpus/texto, correspondientes a **\" háblame, \"** (nótesen los espacios en blanco al inicio y al final):"
      ],
      "metadata": {
        "id": "E0-X60NdElUW"
      },
      "id": "E0-X60NdElUW"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text[:10]"
      ],
      "metadata": {
        "id": "0bOsbsIBEvNe",
        "outputId": "f8b6ffe2-4976-45af-bbed-4d90903ed76f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0bOsbsIBEvNe",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 49, 45, 50, 18, 33, 47, 34, 44, 15]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición del dataset"
      ],
      "metadata": {
        "id": "5CbkUMCIFe0k"
      },
      "id": "5CbkUMCIFe0k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección definimos los datos de entrenamiento y validación."
      ],
      "metadata": {
        "id": "qaN9Nl3bImPX"
      },
      "id": "qaN9Nl3bImPX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se definien las siguientes constantes:\n",
        "- `MAX_CONTEXT_SIZE`: corresponde al tamaño máximo del contexto que se analizará. Se define inicialmente en $100$.\n",
        "- `VALIDATION_SIZE`: tamaño del set de validación. En este caso, se opta por utilizar el $30\\%$."
      ],
      "metadata": {
        "id": "QtVWX-gnItZJ"
      },
      "id": "QtVWX-gnItZJ"
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CONTEXT_SIZE = 100\n",
        "VALIDATION_SIZE = 0.3"
      ],
      "metadata": {
        "id": "iGNdfPN2GL3x"
      },
      "id": "iGNdfPN2GL3x",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define también la cantidad de secuencias de tamaño `MAX_CONTENT_SIZE` que tendrá el set de validación mediante la variable `SEQ_VALIDATION`.\n",
        "\n",
        "Dado a que el tamaño del texto tokenizado es $673064$, el valor de `SEQ_VALIDATION` queda en $2020$, es decir, habrá en el set de validación un total de $2020$ secuencias de tamaño máximo $100$."
      ],
      "metadata": {
        "id": "tkE8NsVVJDku"
      },
      "id": "tkE8NsVVJDku"
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_VALIDATION = int(np.ceil(len(tokenized_text)*VALIDATION_SIZE/MAX_CONTEXT_SIZE))"
      ],
      "metadata": {
        "id": "v8p7s62cGK4V"
      },
      "id": "v8p7s62cGK4V",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realiza la separación del corpus original en `train` (train_text) y `validation` (val_text)."
      ],
      "metadata": {
        "id": "wE99GhzFJ4Y4"
      },
      "id": "wE99GhzFJ4Y4"
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = tokenized_text[:-SEQ_VALIDATION*MAX_CONTEXT_SIZE]\n",
        "val_text = tokenized_text[-SEQ_VALIDATION*MAX_CONTEXT_SIZE:]"
      ],
      "metadata": {
        "id": "2aETP5AKH7Qw"
      },
      "id": "2aETP5AKH7Qw",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences_val = [val_text[init*MAX_CONTEXT_SIZE:init*(MAX_CONTEXT_SIZE+1)] for init in range(SEQ_VALIDATION)]\n",
        "tokenized_sentences_train = [train_text[init:init+MAX_CONTEXT_SIZE] for init in range(len(train_text)-MAX_CONTEXT_SIZE+1)]"
      ],
      "metadata": {
        "id": "Plk44wA0IZBK"
      },
      "id": "Plk44wA0IZBK",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ],
      "metadata": {
        "id": "GDqqzdbQIeT9"
      },
      "id": "GDqqzdbQIeT9",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validamos los tamaños de `X` e `y`."
      ],
      "metadata": {
        "id": "WZK6fNaaLHN2"
      },
      "id": "WZK6fNaaLHN2"
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "leztRfcyJ_4z",
        "outputId": "ab7ed6d7-0830-4886-c92e-173e46cbd423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "leztRfcyJ_4z",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(470964, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "0L1rvZVeLJ7f",
        "outputId": "6e762972-5cfa-4283-964d-3e10177453c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0L1rvZVeLJ7f",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(470964, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}