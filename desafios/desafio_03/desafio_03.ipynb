{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e217d3ee",
      "metadata": {
        "id": "e217d3ee"
      },
      "source": [
        "# Desafío 03"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89133761",
      "metadata": {
        "id": "89133761"
      },
      "source": [
        "## Integrantes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd6a5ae",
      "metadata": {
        "id": "3dd6a5ae"
      },
      "source": [
        "- Acevedo Zain, Gaspar (acevedo.zain.gaspar@gmail.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "947a6938",
      "metadata": {
        "id": "947a6938"
      },
      "source": [
        "## Consignas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090159ca",
      "metadata": {
        "id": "090159ca"
      },
      "source": [
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "***Sugerencias***\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "0ts0cZ-yAt9Q"
      },
      "id": "0ts0cZ-yAt9Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "id": "iTvdqC2eAu0t"
      },
      "id": "iTvdqC2eAu0t",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import bs4 as bs"
      ],
      "metadata": {
        "id": "XwHsZC7zBAtg"
      },
      "id": "XwHsZC7zBAtg",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "MjADsRQKwWxv"
      },
      "id": "MjADsRQKwWxv",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense\n",
        "from keras.models import Model, Sequential"
      ],
      "metadata": {
        "id": "K_oanrmXwZqv"
      },
      "id": "K_oanrmXwZqv",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "kfyGwWsOwcqT"
      },
      "id": "kfyGwWsOwcqT",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "886c1m_JwxRl"
      },
      "id": "886c1m_JwxRl",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selección del corpus"
      ],
      "metadata": {
        "id": "AxXf47bgDCe9"
      },
      "id": "AxXf47bgDCe9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de esta práctica es evaluar modelos de lenguajes con *tokenización por caracteres*, por lo cual, un texto lo suficientemente grande puede servir como `Corpus`.\n",
        "\n",
        "Se elige entonces el libro `La Odisea` de Homero ([source](https://www.textos.info/homero/odisea/ebook)) como `Corpus`."
      ],
      "metadata": {
        "id": "7j9Kc8P5EYmN"
      },
      "id": "7j9Kc8P5EYmN"
    },
    {
      "cell_type": "code",
      "source": [
        "libro_url = \"https://www.textos.info/homero/odisea/ebook\""
      ],
      "metadata": {
        "id": "YpDwlz-1DI2b"
      },
      "id": "YpDwlz-1DI2b",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_html = urllib.request.urlopen(libro_url)\n",
        "raw_html = raw_html.read()"
      ],
      "metadata": {
        "id": "EMjWOkgyDGJZ"
      },
      "id": "EMjWOkgyDGJZ",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se procesa el *html* original mediante la utilidad `bs.BeautifulSoup` a fin de tener el texto en ***un solo string***."
      ],
      "metadata": {
        "id": "1Tl0ibE1Ek6_"
      },
      "id": "1Tl0ibE1Ek6_"
    },
    {
      "cell_type": "code",
      "source": [
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:\n",
        "    article_text += para.text + ' '\n",
        "\n",
        "article_text = article_text.lower()"
      ],
      "metadata": {
        "id": "tDS_17kRDPly"
      },
      "id": "tDS_17kRDPly",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se muestran los primeros $500$ caracteres del texto (corpus)."
      ],
      "metadata": {
        "id": "0KtVmm1pE0Yl"
      },
      "id": "0KtVmm1pE0Yl"
    },
    {
      "cell_type": "code",
      "source": [
        "article_text[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "ebdZ14dJEKtS",
        "outputId": "52b46f7e-70e5-434a-e316-79d5c2b6939b"
      },
      "id": "ebdZ14dJEKtS",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' háblame, musa, de aquel varón de multiforme ingenio que, después de \\ndestruir la sacra ciudad de troya, anduvo peregrinando larguísimo \\ntiempo, vio las poblaciones y conoció las costumbres de muchos hombres y\\n padeció en su ánimo gran número de trabajos en su navegación por el \\nponto, en cuanto procuraba salvar su vida y la vuelta de sus compañeros a\\n la patria. mas ni aun así pudo librarlos, como deseaba, y todos \\nperecieron por sus propias locuras. ¡insensatos! comiéronse las vacas de\\n helios'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el texto se observan algunas *secuencias de escape de caracteres* (salto de línea o `\\n`), por lo cual se las reemplaza a continuación con un caracter vacío."
      ],
      "metadata": {
        "id": "m-iRtdiiFkn5"
      },
      "id": "m-iRtdiiFkn5"
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = str.replace(article_text, \"\\n\", \"\")"
      ],
      "metadata": {
        "id": "Dr18eaNqESzo"
      },
      "id": "Dr18eaNqESzo",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_text[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "INsuGeqCF22a",
        "outputId": "0ef5d25d-f1f4-425b-ec2a-f6f18e979e8d"
      },
      "id": "INsuGeqCF22a",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' háblame, musa, de aquel varón de multiforme ingenio que, después de destruir la sacra ciudad de troya, anduvo peregrinando larguísimo tiempo, vio las poblaciones y conoció las costumbres de muchos hombres y padeció en su ánimo gran número de trabajos en su navegación por el ponto, en cuanto procuraba salvar su vida y la vuelta de sus compañeros a la patria. mas ni aun así pudo librarlos, como deseaba, y todos perecieron por sus propias locuras. ¡insensatos! comiéronse las vacas de helios, hijo '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición del vocabulario + Tokenización"
      ],
      "metadata": {
        "id": "mNB4tuhlAO8Y"
      },
      "id": "mNB4tuhlAO8Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección definiremos nuestro `vocabulario` a partir del corpus original.\n",
        "\n",
        "Luego lo tokenizaremos, a fin de que pueda ser procesado por una Red Neuronal en pasos posteriores."
      ],
      "metadata": {
        "id": "U8sqaDkuFMwF"
      },
      "id": "U8sqaDkuFMwF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos definiendo nuestro vocabulario, que es el conjunto de distintos caracteres que aparecen en nuestro corpus.\n",
        "\n",
        "Para el caso particular del texto seleccionado el tamaño es de $57$.\n",
        "\n",
        "Este valor lo guardamos en la variable `VOCAB_SIZE`."
      ],
      "metadata": {
        "id": "Z68zktzNDgtS"
      },
      "id": "Z68zktzNDgtS"
    },
    {
      "cell_type": "code",
      "source": [
        "char_vocab = set(article_text)\n",
        "VOCAB_SIZE = len(char_vocab)\n",
        "print(f\"Tamaño del vocabulario: {VOCAB_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gonFUApiCVHF",
        "outputId": "04d2a4bc-ca1e-4e9f-e87e-1b27a0dafc65"
      },
      "id": "gonFUApiCVHF",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del vocabulario: 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos dos diccionarios que se utilizarán a lo largo de este trabajo:\n",
        "- `char2idx`: A cada caracter de nuestro vocabulario se le asigna un `índice`.\n",
        "- `idx2char`: Es el *inverso* de `char2idx`, es decir, dado un índice, me devuelve el caracter correspondiente."
      ],
      "metadata": {
        "id": "dVCdbgxXDw6A"
      },
      "id": "dVCdbgxXDw6A"
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {k: v for v,k in enumerate(char_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}"
      ],
      "metadata": {
        "id": "bTzeda8sDHSF"
      },
      "id": "bTzeda8sDHSF",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora `tokenizamos` el corpus. Para ello hacemos uso del diccionario `char2idx`, reemplazando cada caracter por su índice correspondiente."
      ],
      "metadata": {
        "id": "YiZH4Yt7EVJc"
      },
      "id": "YiZH4Yt7EVJc"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = [char2idx[ch] for ch in article_text]"
      ],
      "metadata": {
        "id": "TN0z8xKbEI4H"
      },
      "id": "TN0z8xKbEI4H",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para validar, mostramos primero los $10$ primero caracteres tokenizados del corpus/texto, correspondientes a **\" háblame, \"** (nótesen los espacios en blanco al inicio y al final):"
      ],
      "metadata": {
        "id": "E0-X60NdElUW"
      },
      "id": "E0-X60NdElUW"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bOsbsIBEvNe",
        "outputId": "563c8a04-2d5b-4bfd-92c7-51603bdfa766"
      },
      "id": "0bOsbsIBEvNe",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[14, 35, 54, 30, 45, 11, 2, 10, 15, 14]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición del dataset"
      ],
      "metadata": {
        "id": "5CbkUMCIFe0k"
      },
      "id": "5CbkUMCIFe0k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección definimos los datos de entrenamiento y validación."
      ],
      "metadata": {
        "id": "qaN9Nl3bImPX"
      },
      "id": "qaN9Nl3bImPX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se definien las siguientes constantes:\n",
        "- `MAX_CONTEXT_SIZE`: corresponde al tamaño máximo del contexto que se analizará. Se define inicialmente en $100$.\n",
        "- `P_VAL`: tamaño del set de validación. En este caso, se opta por utilizar el $10\\%$."
      ],
      "metadata": {
        "id": "QtVWX-gnItZJ"
      },
      "id": "QtVWX-gnItZJ"
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CONTEXT_SIZE = 100\n",
        "P_VAL = 0.1"
      ],
      "metadata": {
        "id": "iGNdfPN2GL3x"
      },
      "id": "iGNdfPN2GL3x",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define también la cantidad de secuencias de tamaño `MAX_CONTENT_SIZE` que tendrá el set de validación mediante la variable `NUM_VAL`.\n",
        "\n",
        "Dado a que el tamaño del texto tokenizado es $673064$, el valor de `NUM_VAL` queda en $674$, es decir, habrá en el set de validación un total de $674$ secuencias de tamaño máximo $100$."
      ],
      "metadata": {
        "id": "tkE8NsVVJDku"
      },
      "id": "tkE8NsVVJDku"
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_VAL = int(np.ceil(len(tokenized_text)*P_VAL/MAX_CONTEXT_SIZE))"
      ],
      "metadata": {
        "id": "v8p7s62cGK4V"
      },
      "id": "v8p7s62cGK4V",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realiza la separación del corpus original en `train` (train_text) y `validation` (val_text)."
      ],
      "metadata": {
        "id": "wE99GhzFJ4Y4"
      },
      "id": "wE99GhzFJ4Y4"
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = tokenized_text[:-NUM_VAL * MAX_CONTEXT_SIZE]\n",
        "val_text = tokenized_text[-NUM_VAL * MAX_CONTEXT_SIZE:]"
      ],
      "metadata": {
        "id": "2aETP5AKH7Qw"
      },
      "id": "2aETP5AKH7Qw",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences_val = [val_text[init*MAX_CONTEXT_SIZE:init*(MAX_CONTEXT_SIZE+1)] for init in range(NUM_VAL)]\n",
        "tokenized_sentences_train = [train_text[init:init+MAX_CONTEXT_SIZE] for init in range(len(train_text)-MAX_CONTEXT_SIZE+1)]"
      ],
      "metadata": {
        "id": "Plk44wA0IZBK"
      },
      "id": "Plk44wA0IZBK",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ],
      "metadata": {
        "id": "GDqqzdbQIeT9"
      },
      "id": "GDqqzdbQIeT9",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validamos los tamaños de `X` e `y`."
      ],
      "metadata": {
        "id": "WZK6fNaaLHN2"
      },
      "id": "WZK6fNaaLHN2"
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leztRfcyJ_4z",
        "outputId": "cc3f15c5-c712-4279-dc17-7ee208f25894"
      },
      "id": "leztRfcyJ_4z",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(605564, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L1rvZVeLJ7f",
        "outputId": "1ee86d3a-3c6d-4eee-f7f2-d5f0a239ada3"
      },
      "id": "0L1rvZVeLJ7f",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(605564, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(char_vocab)"
      ],
      "metadata": {
        "id": "B9JoQp9axHLR"
      },
      "id": "B9JoQp9axHLR",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones útiles"
      ],
      "metadata": {
        "id": "KQ8qL-WXUplN"
      },
      "id": "KQ8qL-WXUplN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección se definen una serie de funciones que se utilizarán a lo largo del entrenamiento de los distintos modelos a estudiar."
      ],
      "metadata": {
        "id": "NSMptxs8Urxv"
      },
      "id": "NSMptxs8Urxv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `PplCallback` fue tomada de las notebooks provistas por la materia (Fuente: [Clase 4 - 4_modelo_lenguaje_char.ipynb](https://github.com/gasper-az/procesamiento_lenguaje_natural/blob/main/clase_4/ejercicios/4_modelo_lenguaje_char.ipynb)).\n",
        "\n",
        "La misma permite calcular la métrica de `Perplejidad` al final de cada *epoch* de *entrenamiento*.\n",
        "\n",
        "Se caracteriza además por implementar `Early Stopping` en caso de que la métrica de perplejidad **NO** mejore luego de una cantidad definida de epochs (`patience`, con valore por defecto $5$).\n",
        "\n",
        "También se encarga de guardar el *historial* del perplexity en cada epoch, a fin de poder analizar posteriormente."
      ],
      "metadata": {
        "id": "sXkEF4jgWpHf"
      },
      "id": "sXkEF4jgWpHf"
    },
    {
      "cell_type": "code",
      "source": [
        "class PplCallback(keras.callbacks.Callback):\n",
        "    '''\n",
        "    - Callback ad-hoc para calcular al final de cada epoch de entrenamiento\n",
        "      la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    - Implementa Early Stopping si la perplejidad no mejora después de\n",
        "      `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, history_ppl, patience=5):\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "      self.history_ppl = history_ppl\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "      self.min_score = np.inf\n",
        "      self.patience_counter = 0\n",
        "      self.patience = patience\n",
        "\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(1,len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(1,len_seq)])\n",
        "\n",
        "        if len(subseq)!=0:\n",
        "          self.padded.append(pad_sequences(subseq, maxlen=MAX_CONTEXT_SIZE, padding='pre'))\n",
        "          self.info.append((count,count+len_seq))\n",
        "          count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Perplejidad de cada secuencia\n",
        "        scores = []\n",
        "        predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "        for start, end in self.info:\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores)\n",
        "        self.history_ppl.append(current_score)\n",
        "        print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "        # Early Stopping\n",
        "        if current_score < self.min_score:\n",
        "          self.min_score = current_score\n",
        "          self.model.save(\"my_model.keras\")\n",
        "          print(\"Saved new model!\")\n",
        "          self.patience_counter = 0\n",
        "        else:\n",
        "          self.patience_counter += 1\n",
        "          if self.patience_counter == self.patience:\n",
        "            print(\"Stopping training...\")\n",
        "            self.model.stop_training = True\n"
      ],
      "metadata": {
        "id": "JyDgiT3CVug3"
      },
      "id": "JyDgiT3CVug3",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las siguientes funciones permiten hacer un `encode` y un `decode` de una secuencia dada.\n",
        "\n",
        "Fueron tomadas también de las notebooks provistas por la materia. (Fuente: [Clase 4 - 4_modelo_lenguaje_char.ipynb](https://github.com/gasper-az/procesamiento_lenguaje_natural/blob/main/clase_4/ejercicios/4_modelo_lenguaje_char.ipynb))."
      ],
      "metadata": {
        "id": "iRio7-uJbRWs"
      },
      "id": "iRio7-uJbRWs"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, max_length=MAX_CONTEXT_SIZE):\n",
        "\n",
        "    encoded = [char2idx[ch] for ch in text]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return ''.join([idx2char[ch] for ch in seq])"
      ],
      "metadata": {
        "id": "bN1us4UmbSTF"
      },
      "id": "bN1us4UmbSTF",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `generate_seq` permite, dado un modelo entrenado, generar una secuencia de caracteres. Es decir, funciona de manera autoregresiva.\n",
        "\n",
        "Fue tomada también de las notebooks provistas por la materia. (Fuente: [Clase 4 - 4_modelo_lenguaje_char.ipynb](https://github.com/gasper-az/procesamiento_lenguaje_natural/blob/main/clase_4/ejercicios/4_modelo_lenguaje_char.ipynb))."
      ],
      "metadata": {
        "id": "tUczzKXhdo5m"
      },
      "id": "tUczzKXhdo5m"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq(model, seed_text, max_length, n_words):\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "\t\t# Predicción softmax\n",
        "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        out_word = idx2char[y_hat]\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += out_word\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "lCHlB0Wndc8o"
      },
      "id": "lCHlB0Wndc8o",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las siguientes funciones serán utilizadas al estudiar `BEAM` search.\n",
        "\n",
        "Fueron tomadas también de las notebooks provistas por la materia. (Fuente: [Clase 4 - 4_modelo_lenguaje_char.ipynb](https://github.com/gasper-az/procesamiento_lenguaje_natural/blob/main/clase_4/ejercicios/4_modelo_lenguaje_char.ipynb))."
      ],
      "metadata": {
        "id": "0TcHldXNbufH"
      },
      "id": "0TcHldXNbufH"
    },
    {
      "cell_type": "code",
      "source": [
        "def select_candidates(pred, num_beams, vocab_size, history_probs,\n",
        "                      history_tokens, temp,mode):\n",
        "  pred_large = []\n",
        "\n",
        "  for idx,pp in enumerate(pred):\n",
        "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "  pred_large = np.array(pred_large)\n",
        "\n",
        "  # criterio de selección\n",
        "  if mode == 'det':\n",
        "    # beam search determinista\n",
        "    idx_select = np.argsort(pred_large)[::-1][:num_beams]\n",
        "  elif mode == 'sto':\n",
        "    # beam search con muestreo aleatorio\n",
        "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams,\n",
        "                                  p=softmax(pred_large/temp))\n",
        "  else:\n",
        "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
        "\n",
        "  # traducir a índices de token en el vocabulario\n",
        "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)"
      ],
      "metadata": {
        "id": "cN-pTYUkbrps"
      },
      "id": "cN-pTYUkbrps",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, num_beams, num_words, input, temp=1, mode='det'):\n",
        "    encoded = encode(input)\n",
        "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    history_probs = [0]*num_beams\n",
        "    history_tokens = [encoded[0]]*num_beams\n",
        "\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens,\n",
        "                                        temp,\n",
        "                                        mode)\n",
        "\n",
        "    for i in range(num_words-1):\n",
        "      preds = []\n",
        "      for hist in history_tokens:\n",
        "        input_update = np.array([hist[i+1:]]).copy()\n",
        "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
        "        preds.append(y_hat)\n",
        "      history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens,\n",
        "                                                        temp,\n",
        "                                                        mode)\n",
        "\n",
        "    return history_tokens[:,-(len(input)+num_words):]"
      ],
      "metadata": {
        "id": "NEVyk-l_cl6X"
      },
      "id": "NEVyk-l_cl6X",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo Simple RNN"
      ],
      "metadata": {
        "id": "GXSVm-0MgHDX"
      },
      "id": "GXSVm-0MgHDX"
    },
    {
      "cell_type": "code",
      "source": [
        "# simpleRNN = Sequential()\n",
        "\n",
        "# simpleRNN.add(TimeDistributed(CategoryEncoding(num_tokens=VOCAB_SIZE, output_mode = \"one_hot\"),input_shape=(None,1)))\n",
        "# simpleRNN.add(SimpleRNN(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1 ))\n",
        "# simpleRNN.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "# simpleRNN.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "# simpleRNN.summary()"
      ],
      "metadata": {
        "id": "60REKI6sgtDY"
      },
      "id": "60REKI6sgtDY",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CANT_EPOCHS = 50"
      ],
      "metadata": {
        "id": "iRxMxXvKhdue"
      },
      "id": "iRxMxXvKhdue",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simpleRNN_perplexity_history = []\n",
        "# simpleRNN_history = simpleRNN.fit(X, y, epochs=CANT_EPOCHS, callbacks=[PplCallback(tokenized_sentences_val, simpleRNN_perplexity_history)], batch_size=256)"
      ],
      "metadata": {
        "id": "z-cD7n2Eg9Gh"
      },
      "id": "z-cD7n2Eg9Gh",
      "execution_count": 31,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}