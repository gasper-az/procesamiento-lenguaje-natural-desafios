{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e217d3ee",
   "metadata": {},
   "source": [
    "# Desafío 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89133761",
   "metadata": {},
   "source": [
    "## Integrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd6a5ae",
   "metadata": {},
   "source": [
    "- Acevedo Zain, Gaspar (acevedo.zain.gaspar@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a6938",
   "metadata": {},
   "source": [
    "## Consignas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090159ca",
   "metadata": {},
   "source": [
    "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
    "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
    "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
    "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
    "\n",
    "\n",
    "***Sugerencias***\n",
    "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
    "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
    "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
